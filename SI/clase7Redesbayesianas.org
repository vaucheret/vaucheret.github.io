
#+TITLE: Sistemas Inteligentes
#+DATE:  Claudio Vaucheret
#+AUTHOR: Redes Bayesianas
#+EMAIL: cv@fi.uncoma.edu.ar

#+REVEAL_INIT_OPTIONS:  transition:'slide' 
#+options: toc:1 num:nil

#+REVEAL_THEME: sky
#+REVEAL_HLEVEL: 2
#+reveal_root:  https://cdn.jsdelivr.net/npm/reveal.js

#+REVEAL_EXTRA_CSS: grids.css

* Suposición de Independencia
  
** Motivación

#+REVEAL_HTML: <div style="font-size: 70%;">   
#+ATTR_REVEAL: :frag (roll-in)   
 - Si contemplamos un dominio descripto con $n$ variables Booleanas y
   utilizamos el enfoque de [[color:red][probabilidades conjuntas]],
   se requerirá una tabla de tamaño $O(2^n)$ (exactamente $2^n − 1$
   ¿porqué?). En problemas reales, hay que considerar cientos o miles
   de variables random.
 - [[color:blue][Problema]]

   Cantidad Excesiva de Información a
   mantener con la base de conocimiento. Intratabilidad con
   [[color:red][Complejidad Exponencial]] del cálculo de
   probabilidades conjuntas
 - [[color:blue][Solución: *Independencia*]]

   Detectar proposiciones cuya creencia no
   afecta la creencia de otra proposición

   

** Solución a las Probabilidades Conjuntas

  - [[color:blue][Independencia]]

    Detectar proposiciones cuya creencia no afecta la creencia de otra proposición

  - [[color:blue][Independencia Total es muy rara]]

    Independencia Condicional


**  Ejemplo

 #+REVEAL_HTML: <div style="font-size: 100%;">   
 #+ATTR_REVEAL: :frag (roll-in)
- El hecho de que una persona tenga una enfermedad en el pulmón no
  tiene relación con el color de los ojos de la persona.

- El hecho de que una persona tenga caries no tiene relación con el
  clima.

- Ası́, /no es necesario/ mantener en la base de
  conocimiento información condicional sobre estas proposiciones que
  parecen ser irrelevantes entre sı́.


** Independencia Condicional cuando hay independencia Total

#+REVEAL_HTML: <div style="font-size: 70%;">   
   
- Si $h$ y $e$ son independientes. $$P(h|e) = P(h)$$ y
  
$$P(h, e) = P(h ∧ e) = P(h|e) × P(e) = P(h) × P(e)$$

** Independencia Condicional

#+REVEAL_HTML: <div style="font-size: 70%;">   
 #+ATTR_REVEAL: :frag (roll-in)   
 * Una forma de conocimiento cualitativo puede ser expresada como
   $$P(h|e) = P(h|f ∧ e)$$ Esto significa que $f$ es irrelevante para
   la probabilidad de $h$ dado $e$.

 *  [[color:blue][$$P(carie = true|dolordemuela = true ∧ clima = lluvioso) =$$]]
    [[color:blue][$$P(carie = true|dolordemuela = true)$$]]
    La información sobre el clima es irrelevante en el dominio de las caries.


** Independencia Condicional cuando hay independencia Total

#+REVEAL_HTML: <div style="font-size: 70%;">


- Regla del Producto $$P(h ∧ e) = P(h|e)P(e) = P(e|h)P(h)$$

- Si $h$ y $e$ son independientes. $$P(h|e) = P(h)$$ y $$P(h, e) = P(h ∧ e) = P(h|e) × P(e) = P(h) × P(e)$$

** Propiedades de la Probabilidad Condicional

#+REVEAL_HTML: <div style="font-size: 70%;">
   
- [[color:blue][Regla de la Cadena: sucesivas aplicaciones de la Regla del Producto]]

#+REVEAL_HTML: <div style="font-size: 70%;">

\begin{align}
    P(f_1 ∧ f_2 ∧ \ldots ∧ f_n ) &= P(f_n |f_1 ∧ f_2 ∧ \ldots ∧ f_{n−1} ) × P(f_1 ∧ f_2 ∧ \ldots ∧ f_{n−1} ) \\
&= P(f_1 )× \\
& \hspace{0.7cm}  P(f_2 |f_1 )× \\
& \hspace{0.7cm} P(f_3 |f_1 ∧ f_2 )× \\
& \hspace{0.7cm} \vdots\\
& \hspace{0.7cm} P(f_n |f_1 ∧ f_2 ∧ \ldots ∧ f_{n−1} ) \\
 P(f 1 ∧ f 2 ∧ . . . ∧ f n ) &= \Pi^n_{i=1} P(f_i |f_1 ∧ f_2 ∧ \ldots ∧ f_{i−1} ) \\
\end{align}   


** Independencia Condicional

#+REVEAL_HTML: <div style="font-size: 75%;">
   
- [[color:blue][Definición]]

  Una variable $X$ se dice que es
  [[color:red][independiente]] de la variable aleatoria $Y$ dada la
  variable aleatoria $Z$ , si para todos los valores de esa variable,
  ie para todo $a_i$ , $b_j$ y $c_k$ se cumple: $$P(X = a_i | Y = b_j
  ∧ Z = c_k ) = P(X = a_i | Z = c_k)$$ Esto es, el valor de $Y$ no
  afecta el conocimiento del agente del valor de $X$ dado el valor de
  $Z$.

** Caracterización alternativa de Independencia Condicional

#+REVEAL_HTML: <div style="font-size: 55%;">
   
- [[color:blue][Theorem]]

  Si la variable aleatoria $X$ es independiente de la variable $Y$
  dada la variable aleatoria $Z$, entonces para todo $a_i$ , $b_j$
  , $c_k$ , $$P(X = a_i ∧ Y = b_j | Z = c_k ) = P(X = a_i | Z = c_k )×
  P(Y = b_j | Z = c_k)$$

** Motivación

- Estructura de datos que representa la dependencia entre variables.
- Da una especificación concisa de la distribución de la probabilidad.
  
*Una [[color:blue][Red Bayesiana]] es un grafo dirigido acı́clico en  el que cada nodo es etiquetado con la información cuantitativa  probabilı́stica*

** Red de Creencias o Bayesiana

#+REVEAL_HTML: <div style="font-size: 70%;">

[[file:weathercavity1.png]]   

#+ATTR_REVEAL: :frag (roll-in)         
- [[color:green][Weather]] es independiente de todas las otras variables.
- [[color:green][Cavity]] influencia en forma causal a
  [[color:green][Toothache]] y caries [[color:red][son causa directa]] de dolor y huecos.
- [[color:green][Toothache]] y [[color:green][Catch]] [[color:red][son condicionalmente independientes]] dado [[color:green][Cavity]] $$P(Catch|Toothache, Cavity ) =
  P(Catch|Cavity )$$

** Red de Creencias o Bayesiana (Russell y Norvig)

#+REVEAL_HTML: <div style="font-size: 50%;">

   #+ATTR_HTML:  :height 400    
   [[file:redcrebay.png]]

Nótese que solo se representa una columna en el caso de variables bivaluadas.
En el caso del tiempo podrı́amos haber representado solo 3 valores.

** Influencia Local
   
- [[color:blue][La redes de creencias explotan la propiedad de localidad]]
  
Dada una variable aleatoria $V$ puede existir solo un conjunto
limitado de variables con la propiedad de afectar directamente los
valores que esta variable puede tomar y los valores tomados por
cualquier otra variable no afecta a los valores tomados por $V$.

** Intuición

 #+REVEAL_HTML: <div style="font-size: 70%;">   
 #+ATTR_REVEAL: :frag (roll-in)   
- La topologı́a de la red especifica la relación de independencia condicional que ocurre en el dominio.
- El significado intuitivo de un arco $(X,Y)$ en una red construida
  correctamente es que $X$ tiene influencia directa sobre $Y$.
- En cada red de creencias está implı́cita la suposición de que cada
  variable aleatoria es independiente de aquellas variables aleatorias
  que no son sus descendientes, dados sus padres (evidencia).


** Intuición

 #+REVEAL_HTML: <div style="font-size: 50%;">   
 #+ATTR_REVEAL: :frag (roll-in)   
- Si $X$ es una variable aleatoria con padres en el grafo
  $Y_1,\ldots,Y_n$, toda variable aleatoria que no es descendiente de
  $X$ es independiente de $X$ dados $Y_1,\ldots,Y_n$ $$P(X = a_i | Y_1
  = v_1 ∧ \ldots ∧ Y_n = v_n ∧ R) = P(X = a_i | Y_1 = v_1 ∧ \ldots ∧
  Y_n = v_n)$$ si $R$ no involucra un descendiente de $X$, donde X es
  considerado un descendiente de sı́ mismo. $R$ puede involucrar
  ancestros de $X$ y otros nodos.
- La suposición de independencia establece que toda la influencia de
  los no descendientes es capturada en los valores de los nodos
  padres.
- Modelar con las redes bayesianas requiere la suposición de la
  propiedad de Markov: "No existe ninguna dependencia directa en el
  sistema que se está modelando que no esté explı́citamente
  representada vı́a arcos."


**  Teorema de Bayes
 #+REVEAL_HTML: <div style="font-size: 70%;">
    
Si $P(e|k) \neq 0$ entonces

$$P(h|e ∧ k) = \frac{P(e|h ∧ k) × P (h|k)}{P(e|k)}$$

Si asumimos el conocimiento $k$ como implı́cito y teniendo que $P(e) \neq 0$
entonces

$$P(h|e) = \frac{P(e|h) × P (h)}{P(e)}$$

** Importancia del Teorema de Bayes
 #+REVEAL_HTML: <div style="font-size: 70%;">
   
- Útil para dar probabilidades de diagnósticos a partir de la
  probabilidad causal $$P(Causa|Efecto) = \frac{P(Efecto|Causa) × P(Causa)}{P(Efecto)}$$
- Ejemplo de Alarma: 
  Supongamos que tenemos información sobre la
  confiabilidad del funcionamiento de una alarma y queremos saber que
  tan probable es que haya fuego si suena la alarma. $$P(fuego|alarma)
  = \frac{P(alarma|fuego) × P (fuego)}{P(alarma)}$$

** Ejemplo de Pearl, 1990 (Ver Russell-Norvig)
 #+REVEAL_HTML: <div style="font-size: 70%;">
 - Tenemos una alarma antirrobo instalada en una casa:
   - La alarma salta normalmente con la presencia de ladrones.
   - Pero también cuando ocurren pequeños temblores de tierra
 - Tenemos dos vecinos en la casa, Juan y Marı́a, que han prometido llamar a la policı́a si oyen la alarma:
   - Juan y Marı́a podrı́an no llamar aunque la alarma sonara: por tener música muy alta en su casa, por ejemplo.
   - Incluso podrı́an llamar aunque no hubiera sonado: por confundirla con un teléfono, o con la misma música, por ejemplo.

** Construyendo una red de creencias
#+REVEAL_HTML: <div style="font-size: 70%;">

Para representar una red de creencias necesitamos considerar:

 #+ATTR_REVEAL: :frag (roll-in)   
1. ¿Cuáles son las variables relevantes del problema?
2. ¿Cuáles son las relaciones entre ellas? Esto deberı́a ser expresado
   en términos de influencia local.
3. ¿Qué valores deben tomar estas variables? Esto involucra considerar
   el nivel de detalle en el cual deseamos razonar.
4. ¿De qué manera el valor de una variable depende de las variables
   que la influencian localmente(sus padres)? Esto se expresa en
   términos de las tablas de probabilidad condicional.

** Ejemplo de Pearl, 1990 (Ver Russell-Norvig)
#+REVEAL_HTML: <div style="font-size: 70%;">   

¿Cuáles son los nodos-variables de la Red a representar y cuál su dominio?

| Nombre del Nodo | Tipo     | Valor   |
|-----------------+----------+---------|
| Robo            | Booleano | {T,F}   |
| Terremoto       | Booleano | {T,F}   |
| Alarma          | Booleano | {T,F}   |
| JuanLlama       | Binario  | {Si,No} |
| Marı́aLlama      | Binario  | {Si,No} |

Armemos una topologı́a como una red causal: Esto es de las causas a los
efectos.

** Topologı́a de la Red: Influencia local entre las variables

#+REVEAL_HTML: <div style="font-size: 50%;">

[[file:topred.png]]
   
Nótese que al asumir la propiedad de Markov no hay modo de que un robo
influya sobre el hecho de que Juan llame excepto si tenemos en cuenta si suena
la alarma.

** Algoritmo de Construcción de una Red Bayesiana
#+REVEAL_HTML: <div style="font-size: 70%;">   
   
Supongamos un conjunto de variables aleatorias VARIABLES que
representan un dominio de conocimiento (con incertidumbre) y que las
ordenamos en forma causal (causa-efecto).

[[file:funcionred.png]]

** Red Bayesiana: Especificación de las Tablas de Probabilidades Condicionales

   #+ATTR_HTML:  :height 440    
   [[file:redtablas.png]]
   
** Red Bayesiana de la Alarma
   
Las Probabilidades para $Alarma = False$ son:

| Robo | Terrem | $P(A=False \vert R, T)$ |
|------+--------+-------------------------|
| T    | T      | 0,05                    |
| T    | F      | 0,06                    |
| F    | T      | 0,610                   |
| F    | F      | 0,999                   |


** Observaciones sobre la Red Bayesiana de la Alarma
#+REVEAL_HTML: <div style="font-size: 70%;">      
La topologı́a de la red nos expresa que:

- Robo y Terremoto son causas directas para Alarma
- También, Robo y Terremoto son causas para Juanllama y para
  Marı́allama, pero esa influencia sólo se produce a través de Alarma:
  ni Juan ni Marı́a detectan directamente el robo ni los pequeños
  temblores de tierra
- En la red no se hace referencia directa, por ejemplo, a las causas
  por las cuales Marı́a podrı́a no oı́r la alarma: éstas están implı́citas
  en la tabla de probabilidades $P(Marı́allama—Alarma)$.

** Redes Bayesianas
#+REVEAL_HTML: <div style="font-size: 50%;">

Representan una Distribución de Probabilidades Conjunta y Completa

- Consideremos una red bayesiana con $n$ variables aleatorias y un orden entre esas
   variables:

       $X_1,\ldots,X_n$
- En lo que sigue, supondremos que:
  - $padres(X_i) ⊆ {X_{i−1},\ldots,X_1}$ para esto, basta que el orden escogido
    
    sea consistente con el orden parcial que induce el grafo
  - $P(X_i|X{i−1},\ldots,X_1) = P(X_i|padres(X_i))$ Es decir,
    cada variable es condicionalmente independiente de sus anteriores,
    dados sus
    padres en la red).
- Estas condiciones expresan formalmente nuestra intuición al representar
  nuestro mundo mediante la red bayesiana correspondiente.
  En el ejemplo de la alarma, la red expresa que
  creemos que

  $P(Mariallama|Juanllama,Alarma,Terremoto,Robo) =$
  $P(Mariallama|Alarma)$

** Razonando con las Redes de Creencia
   
La tarea básica de un sistema de inferencia probabilı́stico es:

- Inferencia o Actualización de las Creencias
  Computar la distribución de probabilidad a posteriori para un
  conjunto de variables de consulta, dada nueva información sobre
  las variables de evidencia.

** Tipos de Razonamientos

   [[file:tiposraz.png]]




** Inferencia
   #+REVEAL_HTML: <div style="font-size: 50%;">

Recordemos la Regla de la Cadena:

\begin{align}
    P(X_1 ∧ X_2 ∧ \ldots ∧ X_n ) &=  P(X_1 )× \\
& \hspace{0.7cm}  P(X_2|X_1 )× \\
& \hspace{0.7cm} P(X_3 |X_1 ∧ X_2 )× \\
& \hspace{0.7cm} \vdots\\
& \hspace{0.7cm} P(X_n |X_1 ∧ X_2 ∧ \ldots ∧ X_{n−1} ) \\
  &= \Pi^n_{i=1} P(X_i |X_1 ∧ X_2 ∧ \ldots ∧ X_{i−1} ) \\
\end{align}   

Como en las redes bayesianas el valor de un nodo particular está
condicionado solamente sobre el valor de los nodos padres, entonces

$$P(X_1 ∧ X_2 ∧ \ldots ∧ X_n ) = \Pi^n_{i=1} P(X_i |Padres(X_i))$$

siendo que $Padres(X_i) ⊆ \{X_1 ∧ X_2 ∧ \ldots ∧ X_n\}$

** Inferencia: Ejemplo Alarma
   #+REVEAL_HTML: <div style="font-size: 70%;">
La probabilidad de que la alarma suene, Juan y Marı́a llamen a la
policı́a, pero no haya ocurrido nada es (usamos iniciales, por
simplificar)

\begin{align}
& P(j=Si ∧ m=Si ∧ a=T ∧ r=F ∧ t=F ) \\
& =P(j=Si | a=T)×P(m=Si | a=T)× \\ 
& \hspace{0.8cm} P(a=T | r=F,t=F)×P(r=F)×P(t=F) \\
& = 0, 90 × 0, 70 × 0, 001 × 0, 999 × 0, 998 \\
& = 0, 00062 
\end{align}   

** Redes Bayesianas
 #+REVEAL_HTML: <div style="font-size: 60%;">   
Dominios localmente estructurados:

 #+ATTR_REVEAL: :frag (roll-in)   
- Las relaciones de independencia que existen entre las variables de un dominio hacen que las redes bayesianas sean una representación mucho más compacta y eficiente de una Distribución de Probabilidad completa y conjunta(DCC), que la tabla con todas las posibles combinaciones de valores.
- Además, para un experto en un dominio de conocimiento suele ser más natural dar probabilidades condicionales que directamente las probabilidades de la DCC.
- Con $n$ variables, si cada variable está directamente influenciada por $k$ variables a lo sumo, entonces una red bayesiana necesitarı́a $n2^k$ números, frente a los $2^n$ números de la DCC.
- Hay veces que una variable influye directamente sobre otra, pero esta dependencia es muy tenue. En ese caso, puede compensar no considerar esa dependencia, perdiendo algo de precisión en la representación, pero ganado manejabilidad.

** Inferencia por Casos
 #+REVEAL_HTML: <div style="font-size: 50%;">      
Supongamos que tenemos una variable Booleana Y que es el único padre
de X. Si E no contiene ningún descendiente de X, entonces

\begin{align}
 P(X|E) &= P (X|Y ∧ E) × P(Y |E) + P(X|¬Y ∧ E) × P(¬Y |E) \\
 &= P(X|Y) × P(Y|E) + P (X|¬Y) × P(¬Y|E) \\
 &= P(X|Y ) × P(Y|E) + P(X|¬Y) × (1 − P (Y|E))
\end{align}

Recordemos que la Red de Creencias especifica los valores de $P(X|Y)$ y $P(X|¬Y)$.

Extendemos a múltiples padres $Y_1,\ldots, Y_n$ de $X$, cada $Y_i$ con dominio
$D_i$. Si $E$ no involucra a ninguno de los descendientes de $X$, luego:

\begin{align}
P(X|E) &= \sum_{d ∈ D} P(X|Y = d ∧ E) × P(Y = d|E) \\
       &= \sum_{d ∈ D} P(X|Y = d) × P(Y = d|E) \\
\end{align}

Recordemos que las probabilidades $P(X|Y= d)$ están especificadas en la
red de creencias.  

** Inferencia
 #+REVEAL_HTML: <div style="font-size: 70%;">         
Supongamos que $E$ es una conjunción de $E_1 ∧ E_2$ , donde $E_1$
involucra solamente descendientes de $A$ y $E_2$ no contiene
descendientes de $A$. La Regla de Bayes puede ser utilizada para
calcular $P(A|E)$: $$P(A|E_1 ∧ E_2 ) = \frac{P(E_1|A ∧ E_2) × P(A|E_2)}{P(E_1|E_2)}$$

** Inferencia: Ejemplo
   #+REVEAL_HTML: <div style="font-size: 50%;">         
Calculemos algunas probabilidades
\begin{align} 
P(A = T) &=      P(A = T|R =T ∧ T= T) × P(R=T)×P(T=T) + \\
& \hspace{0.8cm} P(A = T|R =T ∧ T= F) × P(R=T)×P(T=F) + \\
& \hspace{0.8cm} P(A = T|R =F ∧ T= T) × P(R=F)×P(T=T) + \\
& \hspace{0.8cm} P(A = T|R =F ∧ T= F) × P(R=F)×P(T=F)  \\ \\

                &= 0,950 × 0,001 × 0,002 + \\
& \hspace{0.8cm}   0,94 × 0,001 × 0,998 + \\
& \hspace{0.8cm} 0,290 × 0,999 × 0,002 + \\
& \hspace{0.8cm} 0,001 × 0,999 × 0,998 \\ \\

&= 0,0000019 + 0,00093812 + 0,00057942 + 0,000997002 \\
&= 0,002516442 \\
\end{align} 

** Inferencia: Ejemplo
#+REVEAL_HTML: <div style="font-size: 70%;">         
Nótese que como $Robo$ y $Terremoto$ son independientes, entonces

$$P(Robo=T|Terr = T) = P(Robo=T) = 0,001$$
$$P(Terr=T|Robo = T) = P(Terr=T) = 0,002$$

** Inferencia: Ejemplo
   #+REVEAL_HTML: <div style="font-size: 50%;">
  \begin{align} 
  P(J=Si|M = Si) &= P(J = Si|A = T) × P(A=T|M =Si)+ \\
  & \hspace{0.8cm} P(J= Si|A = F) × (1 − P(A=T|M= Si)) \\
  &= 0,90 × 0,1500901175 + 0,05 × (1 − 0,1500901175) \\
  &= 0,1775765999
\end{align}

\begin{align} 
P(A=T|M=Si) &= \frac{P(M=Si|A=T)×P(A=T)}{P(M= Si)} \\
 &= \frac{0,70×0,002516442}{0,011736345} \\
 &= 0, 1500901175
\end{align} 

\begin{align} 
P(M=Si) &= P(M=Si|A =T) × P(A=T) + \\
 & \hspace{0.8cm}  P(M=Si|A=F ) × P(A=F) \\
&= 0,70 × 0,002516442 + 0,01 × (1 − 0,002516442) \\
&= 0,011736345
\end{align}

** Aplicaciones de Redes Bayesianas
     #+REVEAL_HTML: <div style="font-size: 65%;">
- Diagnóstico de problemas de impresora:Printer Troubleshooting Using
  Bayesian Networks. Claus Skaanning- Finn V. Jensen - Uffe
  Kjærulff. Proceedings of the 13th international conference on
  Industrial and engineering applications of artificial intelligence
  and expert systems: Intelligent problem solving: methodologies and
  approaches Pages 367-379,2000.
- Microsoft: Diagnóstico para detectar problemas en impresoras,
  fotocopiadoras,
  autos,etc. https://www.microsoft.com/en-us/research/publication/troubleshooting-under-uncertainty/
- Medicina: Bayesian Network-Based Model for the Diagnosis of
  Deterioration of Semantic Content Compatible with Alzheimer?s
  Disease. José Marı́a Guerrero Triviño- Rafael Martı́nez-Tomás
  Herminia- Peraita Adrados. Foundations on Natural and Artificial
  Computation, 2011. https://link.springer.com/chapter/10.1007/978-3-642-21344-1_44

** Aplicaciones de Redes Bayesianas
        #+REVEAL_HTML: <div style="font-size: 65%;">
- A Bayesian network for IT governance performance prediction. Mårten
  Simonsson- Robert Lagerström - Pontus Johnson. Proceedings of the
  10th international conference on Electronic
  commerce, 2008. https://dl.acm.org/citation.cfm?id=1409542
- NASA: Developing Large-Scale Bayesian Networks by Composition: Fault
  Diagnosis of Electrical Power Systems in Aircraft and
  Spacecraft. Ole J. Mengshoel - Scott Poll - Tolga
  Kurtoglu. https://ntrs.nasa.gov/search.jsp?N=0&Ntk=All&Ntt=bayesian%20network&Ntx=mode%20matchallpartial&Nm=123|
  Collection|NASA%20STI||17|Collection|NACA
- Embedded Bayesian networks for face recognition. Ara V. Nefian, IEEE
  International Conference on Multimedia and
  Expo, 2002. http://www.anefian.com/research/face_reco.htm
